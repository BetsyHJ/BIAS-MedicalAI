{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e247898966a445f2beca46c0400474c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee0e49ece4747eca3f7d44fe41a44c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "root_dir = './NIH-small/sample/'\n",
    "\n",
    "dataset = load_dataset('imagefolder', split='train', data_dir=os.path.join(root_dir, 'images'))\n",
    "# Add a filename column\n",
    "def add_filename(example):\n",
    "    example['filename'] = os.path.basename(example['image'].filename)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_filename)\n",
    "\n",
    "dataset = dataset.cast_column(\"image\", Image(mode=\"RGB\"))\n",
    "\n",
    "# Load the metadata from the CSV file\n",
    "import pandas as pd\n",
    "metadata_file = os.path.join(root_dir, 'sample_labels.csv')\n",
    "# Load the metadata from the CSV file\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Create a dictionary from the metadata for quick lookup\n",
    "metadata_dict = metadata_df.set_index('Image Index').to_dict(orient='index')\n",
    "\n",
    "# Add metadata to the dataset\n",
    "def add_metadata(example):\n",
    "    filename = example['filename']\n",
    "    if filename in metadata_dict:\n",
    "        metadata = metadata_dict[filename]\n",
    "        example.update(metadata)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_metadata)\n",
    "\n",
    "from datasets.features import ClassLabel, Sequence\n",
    "\n",
    "# Split \"Finding Labels\" into multiple labels\n",
    "metadata_df['Finding Labels'] = metadata_df['Finding Labels'].str.split('|')\n",
    "\n",
    "# Get all unique labels\n",
    "all_labels = set(label for sublist in metadata_df['Finding Labels'] for label in sublist)\n",
    "# as no finding label affects so many images, most implementations remove \"no finding\" label.\n",
    "all_labels.remove('No Finding')\n",
    "\n",
    "### #TODO: only select some labels\n",
    "all_labels = set(['Infiltration', 'Effusion', 'Atelectasis', 'Nodule', 'Pneumothorax']) \n",
    "\n",
    "# Create a ClassLabel feature for each unique label\n",
    "class_labels = ClassLabel(names=list(all_labels))\n",
    "\n",
    "# Define the label feature as a sequence of ClassLabel\n",
    "labels_type = Sequence(class_labels)\n",
    "num_labels = len(class_labels.names)\n",
    "\n",
    "\n",
    "# # Remove unnecessary columns if needed\n",
    "# dataset = dataset.remove_columns(['Image Index', 'Finding Labels', 'Follow-up #', 'Patient ID', 'Patient Age', 'Patient Gender'])\n",
    "\n",
    "# Create a dictionary from the metadata for quick lookup\n",
    "metadata_dict = metadata_df.set_index('Image Index').to_dict(orient='index')\n",
    "\n",
    "# Add metadata to the dataset, including the sequence of class labels\n",
    "def add_metadata(example):\n",
    "    filename = example['filename']\n",
    "    if filename in metadata_dict:\n",
    "        metadata = metadata_dict[filename]\n",
    "        example.update(metadata)\n",
    "        # example['labels_list'] = [class_labels.str2int(label) if label in class_labels.names else 'No Finding' for label in metadata['Finding Labels']]\n",
    "        example['labels'] = [float(class_labels.int2str(x) in metadata['Finding Labels']) for x in range(num_labels)]\n",
    "    return example\n",
    "\n",
    "# Apply the metadata and features to the dataset\n",
    "dataset = dataset.map(add_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9111607ecad48ddb3e2a6a2e5cfd998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5606 2065\n"
     ]
    }
   ],
   "source": [
    "# # filter data with no finding label; we can also down-sample it.\n",
    "dataset_only_finding = dataset.filter(lambda example: sum(example['labels']) >= 1.0)\n",
    "print(len(dataset), len(dataset_only_finding))\n",
    "dataset = dataset_only_finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data split\n",
    "train : valid : test with ratio of 6:2:2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.4, seed=42)\n",
    "train_ds = train_testvalid['train']\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "val_ds = test_valid['train']\n",
    "test_ds = test_valid['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "We will now preprocess the data. The model requires 2 things: pixel_values and labels.\n",
    "\n",
    "We will perform data augmentaton on-the-fly using HuggingFace Datasets' set_transform method (docs can be found here). This method is kind of a lazy map: the transform is only applied when examples are accessed. This is convenient for tokenizing or padding text, or augmenting images at training time for example, as we will do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "size = 224\n",
    "\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            # RandomResizedCrop(size),\n",
    "            # RandomHorizontalFlip(),\n",
    "            Resize(size),\n",
    "            ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            # CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.transforms import (\n",
    "#     Activations,\n",
    "#     EnsureChannelFirst,\n",
    "#     AsDiscrete,\n",
    "#     Compose,\n",
    "#     LoadImage,\n",
    "#     RandFlip,\n",
    "#     RandRotate,\n",
    "#     RandZoom,\n",
    "#     ScaleIntensity,\n",
    "# )\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # # following: https://github.com/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb\n",
    "\n",
    "# _train_transforms = Compose(\n",
    "#     [\n",
    "#         RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "#         RandFlip(spatial_axis=0, prob=0.5),\n",
    "#         RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# _val_transforms = Compose([])\n",
    "\n",
    "# def train_transforms(examples):\n",
    "#     examples['pixel_values'] = [_train_transforms(image) for image in examples['image']]\n",
    "#     return examples\n",
    "\n",
    "# def val_transforms(examples):\n",
    "#     examples['pixel_values'] = [_val_transforms(image) for image in examples['image']]\n",
    "#     return examples\n",
    "\n",
    "# # Set the transforms\n",
    "# train_ds.set_transform(train_transforms)\n",
    "# val_ds.set_transform(val_transforms)\n",
    "# test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.transforms import LoadImageD, EnsureChannelFirstD, ScaleIntensityD, Compose\n",
    "\n",
    "# transform = Compose(\n",
    "#     [\n",
    "#         LoadImageD(keys=\"image\", image_only=True),\n",
    "#         EnsureChannelFirstD(keys=\"image\"),\n",
    "#         ScaleIntensityD(keys=\"image\"),\n",
    "#     ]\n",
    "# )\n",
    "# transform(train_ds[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples]).to(device)\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples]).to(device).float() # change for one-hot multilabels\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 224, 224])\n",
      "labels torch.Size([4, 5])\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)\n",
    "    if k == 'labels':\n",
    "      print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Define the model\n",
    "class ResNetMultiLabel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetMultiLabel, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNetMultiLabel(num_labels).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_freq(ground_labels):\n",
    "#     num_samples = ground_labels.shape[0]\n",
    "#     pos_samples = np.sum(ground_labels,axis=0)\n",
    "#     neg_samples = num_samples-pos_samples\n",
    "#     pos_samples = pos_samples/float(num_samples)\n",
    "#     neg_samples = neg_samples/float(num_samples)\n",
    "#     return pos_samples, neg_samples\n",
    "\n",
    "# ground_labels = []\n",
    "# for i in train_ds:\n",
    "#     ground_labels.append(i['labels'])\n",
    "# ground_labels = np.array(ground_labels)\n",
    "# print(ground_labels.shape)\n",
    "# freq_pos, freq_neg = compute_freq(ground_labels)\n",
    "\n",
    "# freq_pos, freq_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "EPOCH 1::   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeb0397912f44b381bf967246a1908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 2::  10%|█         | 1/10 [00:49<07:21, 49.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.5073820292949677 valid 0.7439332604408264 valid_roc_auc70.75150814463316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5718f6ad51c43dfb04060e6c3dd67b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 3::  20%|██        | 2/10 [01:38<06:32, 49.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.2960722461342812 valid 0.7433245778083801 valid_roc_auc71.67767891830391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f946d514a654de8a3542c527e9844c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 4::  30%|███       | 3/10 [02:25<05:37, 48.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.16382842659950256 valid 0.7385877966880798 valid_roc_auc71.92209061771561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9534ecf1ec451890b465e7c53c1f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 5::  40%|████      | 4/10 [03:10<04:42, 47.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.13172145262360574 valid 0.7303011417388916 valid_roc_auc73.24285457597958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7175f90a1c52464eab8418e96e3f0080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 6::  50%|█████     | 5/10 [03:56<03:52, 46.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.13462840691208838 valid 0.7256736159324646 valid_roc_auc73.25528443778444\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b84dba46fe54fbbbfb189aea9ee288a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 7::  60%|██████    | 6/10 [04:41<03:04, 46.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.08356831427663565 valid 0.722332239151001 valid_roc_auc73.9336976911977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ec6e77c9a2485bbb80e1a68b8b64fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 8::  70%|███████   | 7/10 [05:26<02:17, 45.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.09317317251116038 valid 0.7263154983520508 valid_roc_auc74.54063818126318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70552a27879431888e664b763185dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 9::  80%|████████  | 8/10 [06:11<01:31, 45.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.07238347614184022 valid 0.7274338603019714 valid_roc_auc76.15835567210566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c59594a2a8457e9fd7b7762b9aab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 10::  90%|█████████ | 9/10 [06:57<00:45, 45.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.04813537690788507 valid 0.7241875529289246 valid_roc_auc74.12788003662996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab3f1637d4947d9a0c2e84872110b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 10:: 100%|██████████| 10/10 [07:43<00:00, 46.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.049435443803668024 valid 0.7280676960945129 valid_roc_auc75.66183864746365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# # Loss function and optimizer; @TODO: Alternative way is to find the best thresholds for labels on the validation set.\n",
    "# weights = np.array(freq_neg, dtype=np.float32) / np.array(freq_pos, dtype=np.float32)\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.\n",
    "    pbar = tqdm(enumerate(train_dataloader), unit=\"batch\", total=len(train_dataloader))\n",
    "    for i, data in pbar:\n",
    "        inputs, labels = data['pixel_values'], data['labels']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description('  batch {} loss: {}'.format(i + 1, loss.item()))\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            # pbar.set_description('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('logs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "pbar = trange(EPOCHS)\n",
    "for epoch in pbar:\n",
    "    pbar.set_description('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata['pixel_values'], vdata['labels']\n",
    "            voutputs = sigmoid(model(vinputs))\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            roc_auc += roc_auc_score(vlabels.cpu().numpy(), voutputs.cpu().numpy(), average = 'micro')\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {} valid_roc_auc {}'.format(avg_loss, avg_vloss, roc_auc / len(val_dataloader)))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Testds \n",
    "Consider metrics for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e67119ec5c4e568904499dade27b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Pneumothorax       0.45      0.22      0.29        60\n",
      " Atelectasis       0.58      0.33      0.42        98\n",
      "      Nodule       0.29      0.25      0.26        65\n",
      "Infiltration       0.55      0.70      0.62       183\n",
      "    Effusion       0.50      0.53      0.51       114\n",
      "\n",
      "   micro avg       0.51      0.48      0.49       520\n",
      "   macro avg       0.47      0.40      0.42       520\n",
      "weighted avg       0.50      0.48      0.48       520\n",
      " samples avg       0.46      0.50      0.46       520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.4925816023738872,\n",
       " 'roc_auc': 0.7435499128703013,\n",
       " 'accuracy': 0.26150121065375304}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import EvalPrediction\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, probs.cpu().numpy(), average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "\n",
    "    print(classification_report(y_true=y_true.astype(int), y_pred=y_pred, target_names=class_labels.names))\n",
    "    # labels = train_ds.features['labels_list']\n",
    "    # cm = confusion_matrix(y_true, y_pred)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    # disp.plot(xticks_rotation=45)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# y_true = outputs.label_ids\n",
    "# y_pred = outputs.predictions #.argmax(1)\n",
    "\n",
    "# multi_label_metrics(y_pred, y_true)\n",
    "\n",
    "model.eval()\n",
    "y_true = torch.tensor([], dtype=torch.long)\n",
    "y_pred = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=32)\n",
    "    for i, vdata in tqdm(enumerate(test_dataloader), unit=\"batch\", total=len(test_dataloader)):\n",
    "        vinputs, vlabels = vdata['pixel_values'], vdata['labels'].cpu()\n",
    "        voutputs = model(vinputs).cpu()\n",
    "        y_pred = torch.cat((y_pred, voutputs), 0)\n",
    "        y_true = torch.cat((y_true, vlabels), 0)\n",
    "multi_label_metrics(y_pred, y_true.numpy())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3430c2d40a4847c4a6049cdb62182800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Pneumothorax       0.96      0.85      0.90       154\n",
      " Atelectasis       0.89      0.88      0.89       305\n",
      "      Nodule       0.80      0.86      0.83       185\n",
      "Infiltration       0.85      0.93      0.89       607\n",
      "    Effusion       0.85      0.92      0.88       387\n",
      "\n",
      "   micro avg       0.86      0.90      0.88      1638\n",
      "   macro avg       0.87      0.89      0.88      1638\n",
      "weighted avg       0.86      0.90      0.88      1638\n",
      " samples avg       0.85      0.90      0.86      1638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8796185935637664,\n",
       " 'roc_auc': 0.9759203929710842,\n",
       " 'accuracy': 0.7280064568200162}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(test_dataloader):\n",
    "    y_true = torch.tensor([], dtype=torch.long)\n",
    "    y_pred = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in tqdm(enumerate(test_dataloader), unit=\"batch\", total=len(test_dataloader)):\n",
    "            vinputs, vlabels = vdata['pixel_values'], vdata['labels'].cpu()\n",
    "            voutputs = model(vinputs).cpu()\n",
    "            y_pred = torch.cat((y_pred, voutputs), 0)\n",
    "            y_true = torch.cat((y_true, vlabels), 0)\n",
    "    return multi_label_metrics(y_pred, y_true.numpy())\n",
    "evaluate(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
