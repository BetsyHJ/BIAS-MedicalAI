{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2601d1d65e4702a64f1e5ded5d814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9117c542ed45f69d2cac53deded15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "root_dir = './NIH-small/sample/'\n",
    "\n",
    "dataset = load_dataset('imagefolder', split='train', data_dir=os.path.join(root_dir, 'images'))\n",
    "# Add a filename column\n",
    "def add_filename(example):\n",
    "    example['filename'] = os.path.basename(example['image'].filename)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_filename)\n",
    "\n",
    "dataset = dataset.cast_column(\"image\", Image(mode=\"RGB\"))\n",
    "\n",
    "# Load the metadata from the CSV file\n",
    "import pandas as pd\n",
    "metadata_file = os.path.join(root_dir, 'sample_labels.csv')\n",
    "# Load the metadata from the CSV file\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Create a dictionary from the metadata for quick lookup\n",
    "metadata_dict = metadata_df.set_index('Image Index').to_dict(orient='index')\n",
    "\n",
    "# Add metadata to the dataset\n",
    "def add_metadata(example):\n",
    "    filename = example['filename']\n",
    "    if filename in metadata_dict:\n",
    "        metadata = metadata_dict[filename]\n",
    "        example.update(metadata)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_metadata)\n",
    "\n",
    "from datasets.features import ClassLabel, Sequence\n",
    "\n",
    "# Split \"Finding Labels\" into multiple labels\n",
    "metadata_df['Finding Labels'] = metadata_df['Finding Labels'].str.split('|')\n",
    "\n",
    "# Get all unique labels\n",
    "all_labels = set(label for sublist in metadata_df['Finding Labels'] for label in sublist)\n",
    "# as no finding label affects so many images, most implementations remove \"no finding\" label.\n",
    "all_labels.remove('No Finding')\n",
    "\n",
    "# ### #TODO: only select some labels\n",
    "# all_labels = set(['Infiltration', 'Effusion', 'Atelectasis', 'Nodule', 'Pneumothorax']) \n",
    "\n",
    "# Create a ClassLabel feature for each unique label\n",
    "class_labels = ClassLabel(names=list(all_labels))\n",
    "\n",
    "# Define the label feature as a sequence of ClassLabel\n",
    "labels_type = Sequence(class_labels)\n",
    "num_labels = len(class_labels.names)\n",
    "\n",
    "\n",
    "# # Remove unnecessary columns if needed\n",
    "# dataset = dataset.remove_columns(['Image Index', 'Finding Labels', 'Follow-up #', 'Patient ID', 'Patient Age', 'Patient Gender'])\n",
    "\n",
    "# Create a dictionary from the metadata for quick lookup\n",
    "metadata_dict = metadata_df.set_index('Image Index').to_dict(orient='index')\n",
    "\n",
    "# Add metadata to the dataset, including the sequence of class labels\n",
    "def add_metadata(example):\n",
    "    filename = example['filename']\n",
    "    if filename in metadata_dict:\n",
    "        metadata = metadata_dict[filename]\n",
    "        example.update(metadata)\n",
    "        # example['labels_list'] = [class_labels.str2int(label) if label in class_labels.names else 'No Finding' for label in metadata['Finding Labels']]\n",
    "        example['labels'] = [float(class_labels.int2str(x) in metadata['Finding Labels']) for x in range(num_labels)]\n",
    "    return example\n",
    "\n",
    "# Apply the metadata and features to the dataset\n",
    "dataset = dataset.map(add_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a61b0966b2462986946d5f4bcf15f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5606 2562\n"
     ]
    }
   ],
   "source": [
    "# # filter data with no finding label; we can also down-sample it.\n",
    "dataset_only_finding = dataset.filter(lambda example: sum(example['labels']) >= 1.0)\n",
    "print(len(dataset), len(dataset_only_finding))\n",
    "dataset = dataset_only_finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data split\n",
    "train : valid : test with ratio of 6:2:2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.4, seed=42)\n",
    "train_ds = train_testvalid['train']\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "val_ds = test_valid['train']\n",
    "test_ds = test_valid['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "We will now preprocess the data. The model requires 2 things: pixel_values and labels.\n",
    "\n",
    "We will perform data augmentaton on-the-fly using HuggingFace Datasets' set_transform method (docs can be found here). This method is kind of a lazy map: the transform is only applied when examples are accessed. This is convenient for tokenizing or padding text, or augmenting images at training time for example, as we will do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "size = 224\n",
    "\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            # RandomResizedCrop(size),\n",
    "            # RandomHorizontalFlip(),\n",
    "            Resize(size),\n",
    "            ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            # CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            # normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.transforms import (\n",
    "#     Activations,\n",
    "#     EnsureChannelFirst,\n",
    "#     AsDiscrete,\n",
    "#     Compose,\n",
    "#     LoadImage,\n",
    "#     RandFlip,\n",
    "#     RandRotate,\n",
    "#     RandZoom,\n",
    "#     ScaleIntensity,\n",
    "# )\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # # following: https://github.com/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb\n",
    "\n",
    "# _train_transforms = Compose(\n",
    "#     [\n",
    "#         RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "#         RandFlip(spatial_axis=0, prob=0.5),\n",
    "#         RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# _val_transforms = Compose([])\n",
    "\n",
    "# def train_transforms(examples):\n",
    "#     examples['pixel_values'] = [_train_transforms(image) for image in examples['image']]\n",
    "#     return examples\n",
    "\n",
    "# def val_transforms(examples):\n",
    "#     examples['pixel_values'] = [_val_transforms(image) for image in examples['image']]\n",
    "#     return examples\n",
    "\n",
    "# # Set the transforms\n",
    "# train_ds.set_transform(train_transforms)\n",
    "# val_ds.set_transform(val_transforms)\n",
    "# test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from monai.transforms import LoadImageD, EnsureChannelFirstD, ScaleIntensityD, Compose\n",
    "\n",
    "# transform = Compose(\n",
    "#     [\n",
    "#         LoadImageD(keys=\"image\", image_only=True),\n",
    "#         EnsureChannelFirstD(keys=\"image\"),\n",
    "#         ScaleIntensityD(keys=\"image\"),\n",
    "#     ]\n",
    "# )\n",
    "# transform(train_ds[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples]).to(device)\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples]).to(device).float() # change for one-hot multilabels\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 224, 224])\n",
      "labels torch.Size([4, 14])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)\n",
    "    if k == 'labels':\n",
    "      print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Define the model\n",
    "class ResNetMultiLabel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetMultiLabel, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNetMultiLabel(num_labels).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_freq(ground_labels):\n",
    "#     num_samples = ground_labels.shape[0]\n",
    "#     pos_samples = np.sum(ground_labels,axis=0)\n",
    "#     neg_samples = num_samples-pos_samples\n",
    "#     pos_samples = pos_samples/float(num_samples)\n",
    "#     neg_samples = neg_samples/float(num_samples)\n",
    "#     return pos_samples, neg_samples\n",
    "\n",
    "# ground_labels = []\n",
    "# for i in train_ds:\n",
    "#     ground_labels.append(i['labels'])\n",
    "# ground_labels = np.array(ground_labels)\n",
    "# print(ground_labels.shape)\n",
    "# freq_pos, freq_neg = compute_freq(ground_labels)\n",
    "\n",
    "# freq_pos, freq_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "EPOCH 1::   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf014b49bf64ee4948bf620707de8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 2::  10%|█         | 1/10 [00:57<08:41, 57.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.30316309332847596 valid 0.28858596086502075 valid_roc_auc 0.7997950804380415 valid_f1 0.1855411949161948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3994e779efa14c0eb89523eac379e4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 3::  20%|██        | 2/10 [01:54<07:34, 56.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.223697267472744 valid 0.311373233795166 valid_roc_auc 0.7821239074953338 valid_f1 0.2607343496474928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3461c68ca24509a5eb812062f7c5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 4::  30%|███       | 3/10 [02:49<06:34, 56.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.09933514297008514 valid 0.3459261655807495 valid_roc_auc 0.7686235991023168 valid_f1 0.3043338312524893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71957f22dd4473bbaa987053c31fd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 5::  40%|████      | 4/10 [03:47<05:40, 56.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.07085093036293984 valid 0.40062621235847473 valid_roc_auc 0.6961210545090329 valid_f1 0.23949041409978902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b654f10fefcf40738de64e6083cac571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 6::  50%|█████     | 5/10 [04:43<04:43, 56.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.05003390647470951 valid 0.3828139007091522 valid_roc_auc 0.739586269727854 valid_f1 0.3015326622766637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51e0362ee1144939bb44c93ff5076b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 7::  60%|██████    | 6/10 [05:39<03:45, 56.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.041638802736997604 valid 0.40543124079704285 valid_roc_auc 0.7414713231456472 valid_f1 0.26870254311660546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0cee0b75844792a323a367888b2097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 8::  70%|███████   | 7/10 [06:36<02:49, 56.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.0338793683797121 valid 0.42156872153282166 valid_roc_auc 0.7500112944913842 valid_f1 0.31191944544426137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d259d6d976e4103af9e08834694e562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 9::  80%|████████  | 8/10 [07:33<01:53, 56.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.025895066931843756 valid 0.43611279129981995 valid_roc_auc 0.730475573659464 valid_f1 0.30953201720664947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e896ee1229a453094b7fac3ec9c31f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 10::  90%|█████████ | 9/10 [08:30<00:56, 56.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.02404321953654289 valid 0.4388706088066101 valid_roc_auc 0.7229068950284728 valid_f1 0.3045433987989132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca95c65399b4207abddeb2ec17f1f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 10:: 100%|██████████| 10/10 [09:27<00:00, 56.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.0179381319321692 valid 0.4592209756374359 valid_roc_auc 0.7186163536604526 valid_f1 0.304054050779694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# # Loss function and optimizer; @TODO: Alternative way is to find the best thresholds for labels on the validation set.\n",
    "# weights = np.array(freq_neg, dtype=np.float32) / np.array(freq_pos, dtype=np.float32)\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.\n",
    "    pbar = tqdm(enumerate(train_dataloader), unit=\"batch\", total=len(train_dataloader))\n",
    "    for i, data in pbar:\n",
    "        inputs, labels = data['pixel_values'], data['labels']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description('  batch {} loss: {}'.format(i + 1, loss.item()))\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            # pbar.set_description('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('logs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "pbar = trange(EPOCHS)\n",
    "for epoch in pbar:\n",
    "    pbar.set_description('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    roc_auc, f1 = 0.0, 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata['pixel_values'], vdata['labels']\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            vprobs = torch.sigmoid(voutputs).cpu().numpy()\n",
    "            y_preds = np.zeros(vprobs.shape)\n",
    "            y_preds[np.where(vprobs >= 0.5)] = 1\n",
    "            roc_auc += roc_auc_score(vlabels.cpu().numpy(), vprobs, average = 'micro')\n",
    "            f1 += f1_score(vlabels.cpu().numpy(), y_preds, average = 'micro')\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {} valid_roc_auc {} valid_f1 {}'.format(avg_loss, avg_vloss, roc_auc / len(val_dataloader), f1 / len(val_dataloader)))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Testds \n",
    "Consider metrics for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import EvalPrediction\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5, verbose=1):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    probs = torch.sigmoid(predictions).cpu().numpy()\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    if verbose:\n",
    "        print(classification_report(y_true=y_true.astype(int), y_pred=y_pred, target_names=class_labels.names))\n",
    "    # labels = train_ds.features['labels_list']\n",
    "    # cm = confusion_matrix(y_true, y_pred)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    # disp.plot(xticks_rotation=45)\n",
    "\n",
    "    return metrics\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874ec5b3541943e38328d6b7440634cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal thresholds for each category: [0.06 0.02 0.04 0.11 0.12 0.37 0.36 0.01 0.5  0.44 0.04 0.07 0.01 0.16]\n"
     ]
    }
   ],
   "source": [
    "## Tuning threshold on val_ds: https://vitaliset.github.io/threshold-dependent-opt/ on f_1 metric\n",
    "## TODO: we can also use roc_curve to decide the best threshold: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "def optimize_threshold_metric(model, val_dataloader, threshold_grid=None):\n",
    "    y_true = torch.tensor([], dtype=torch.long)\n",
    "    predictions = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in tqdm(enumerate(val_dataloader), unit=\"batch\", total=len(val_dataloader)):\n",
    "            vinputs, vlabels = vdata['pixel_values'], vdata['labels'].cpu()\n",
    "            voutputs = model(vinputs).cpu()\n",
    "            predictions = torch.cat((predictions, voutputs), 0)\n",
    "            y_true = torch.cat((y_true, vlabels), 0)\n",
    "\n",
    "    if threshold_grid is None:\n",
    "        threshold_grid = np.arange(0.01, 1, 0.01)\n",
    "    optimal_thresholds = np.zeros(num_labels)\n",
    "    for i in range(num_labels):\n",
    "        best_threshold = 0.5\n",
    "        best_f1 = 0\n",
    "        for threshold in threshold_grid:\n",
    "            probs = torch.sigmoid(predictions).cpu().numpy()\n",
    "            # next, use threshold to turn them into integer predictions\n",
    "            y_pred = np.zeros(probs.shape)\n",
    "            y_pred[np.where(probs >= threshold)] = 1\n",
    "            f1 = f1_score(y_true[:, i], y_pred[:, i], average='binary')\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        optimal_thresholds[i] = best_threshold\n",
    "    print(\"Optimal thresholds for each category:\", optimal_thresholds)\n",
    "    return optimal_thresholds\n",
    "\n",
    "\n",
    "thresholds = optimize_threshold_metric(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e52b93d4e3465a9de743bfad3c9c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Fibrosis       0.00      0.00      0.00        20\n",
      "         Emphysema       0.20      0.46      0.28        28\n",
      "     Consolidation       0.10      0.26      0.14        47\n",
      "             Edema       0.10      0.24      0.14        25\n",
      "            Nodule       0.19      0.24      0.21        59\n",
      "      Cardiomegaly       0.47      0.25      0.33        28\n",
      "      Pneumothorax       0.38      0.24      0.30        66\n",
      "          Effusion       0.37      0.62      0.47       133\n",
      "            Hernia       0.00      0.00      0.00         5\n",
      "              Mass       0.25      0.12      0.16        68\n",
      "       Atelectasis       0.35      0.55      0.43       107\n",
      "Pleural_Thickening       0.06      0.08      0.06        39\n",
      "         Pneumonia       0.03      0.36      0.05        11\n",
      "      Infiltration       0.41      0.73      0.52       187\n",
      "\n",
      "         micro avg       0.26      0.44      0.33       823\n",
      "         macro avg       0.21      0.30      0.22       823\n",
      "      weighted avg       0.30      0.44      0.34       823\n",
      "       samples avg       0.28      0.45      0.32       823\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.32909090909090905,\n",
       " 'roc_auc': 0.7106333347154663,\n",
       " 'accuracy': 0.056530214424951264}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(test_dataloader, threshold=0.5, verbose=1):\n",
    "    y_true = torch.tensor([], dtype=torch.long)\n",
    "    y_pred = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in tqdm(enumerate(test_dataloader), unit=\"batch\", total=len(test_dataloader)):\n",
    "            vinputs, vlabels = vdata['pixel_values'], vdata['labels'].cpu()\n",
    "            voutputs = model(vinputs).cpu()\n",
    "            y_pred = torch.cat((y_pred, voutputs), 0)\n",
    "            y_true = torch.cat((y_true, vlabels), 0)\n",
    "    return multi_label_metrics(y_pred, y_true.numpy(), threshold=threshold, verbose=verbose)\n",
    "\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=32)\n",
    "evaluate(test_dataloader, threshold=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65421478dc5848febe07af0f834fb061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Fibrosis       0.00      0.00      0.00        20\n",
      "         Emphysema       1.00      0.11      0.19        28\n",
      "     Consolidation       0.18      0.06      0.09        47\n",
      "             Edema       0.18      0.08      0.11        25\n",
      "            Nodule       0.35      0.10      0.16        59\n",
      "      Cardiomegaly       0.70      0.25      0.37        28\n",
      "      Pneumothorax       0.43      0.14      0.21        66\n",
      "          Effusion       0.45      0.20      0.28       133\n",
      "            Hernia       0.00      0.00      0.00         5\n",
      "              Mass       0.28      0.12      0.16        68\n",
      "       Atelectasis       0.42      0.19      0.26       107\n",
      "Pleural_Thickening       0.33      0.03      0.05        39\n",
      "         Pneumonia       0.00      0.00      0.00        11\n",
      "      Infiltration       0.45      0.55      0.49       187\n",
      "\n",
      "         micro avg       0.42      0.23      0.30       823\n",
      "         macro avg       0.34      0.13      0.17       823\n",
      "      weighted avg       0.40      0.23      0.26       823\n",
      "       samples avg       0.31      0.26      0.26       823\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.2958300550747443,\n",
       " 'roc_auc': 0.7106333347154663,\n",
       " 'accuracy': 0.1539961013645224}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_dataloader, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on fairness metrics\n",
    "E.g., performance difference between female and male groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28dd098f6244cc993e1febb3fca6389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e174f8736da42c88ed9c18784fda7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ds_female = test_ds.filter(lambda example: example['Patient Gender'] == 'F')\n",
    "test_ds_male = test_ds.filter(lambda example: example['Patient Gender'] == 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd6caa2927e4e2d90d387052f0a4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Fibrosis       0.00      0.00      0.00        11\n",
      "         Emphysema       0.15      0.33      0.21        12\n",
      "     Consolidation       0.04      0.12      0.05        17\n",
      "             Edema       0.12      0.23      0.16        13\n",
      "            Nodule       0.18      0.16      0.17        32\n",
      "      Cardiomegaly       0.40      0.33      0.36        12\n",
      "      Pneumothorax       0.33      0.27      0.30        33\n",
      "          Effusion       0.35      0.69      0.47        52\n",
      "            Hernia       0.00      0.00      0.00         2\n",
      "              Mass       0.30      0.11      0.16        28\n",
      "       Atelectasis       0.25      0.46      0.32        35\n",
      "Pleural_Thickening       0.00      0.00      0.00        21\n",
      "         Pneumonia       0.03      0.29      0.06         7\n",
      "      Infiltration       0.43      0.70      0.53        81\n",
      "\n",
      "         micro avg       0.24      0.40      0.30       356\n",
      "         macro avg       0.18      0.26      0.20       356\n",
      "      weighted avg       0.27      0.40      0.31       356\n",
      "       samples avg       0.24      0.41      0.28       356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.2977824709609292,\n",
       " 'roc_auc': 0.6963209596289522,\n",
       " 'accuracy': 0.036036036036036036}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader_female = DataLoader(test_ds_female, collate_fn=collate_fn, batch_size=32)\n",
    "evaluate(test_dataloader_female, threshold=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce43867bbf7435a911da5dfb05c4cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Fibrosis       0.00      0.00      0.00         9\n",
      "         Emphysema       0.23      0.56      0.33        16\n",
      "     Consolidation       0.14      0.33      0.20        30\n",
      "             Edema       0.09      0.25      0.13        12\n",
      "            Nodule       0.20      0.33      0.25        27\n",
      "      Cardiomegaly       0.60      0.19      0.29        16\n",
      "      Pneumothorax       0.47      0.21      0.29        33\n",
      "          Effusion       0.39      0.58      0.47        81\n",
      "            Hernia       0.00      0.00      0.00         3\n",
      "              Mass       0.23      0.12      0.16        40\n",
      "       Atelectasis       0.41      0.60      0.49        72\n",
      "Pleural_Thickening       0.10      0.17      0.12        18\n",
      "         Pneumonia       0.02      0.50      0.04         4\n",
      "      Infiltration       0.39      0.75      0.51       106\n",
      "\n",
      "         micro avg       0.28      0.47      0.35       467\n",
      "         macro avg       0.23      0.33      0.23       467\n",
      "      weighted avg       0.33      0.47      0.36       467\n",
      "       samples avg       0.30      0.49      0.34       467\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/huangjin/opt/anaconda3/envs/ViT/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.35275339185953714,\n",
       " 'roc_auc': 0.7220975868359703,\n",
       " 'accuracy': 0.07216494845360824}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader_male = DataLoader(test_ds_male, collate_fn=collate_fn, batch_size=32)\n",
    "evaluate(test_dataloader_male, threshold=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7,  19,  29,  46,  56,  58,  67,  68,  73, 127, 146, 163, 192,\n",
       "       217, 220, 237, 239, 245, 255, 297, 304, 330, 383, 390, 404, 452,\n",
       "       461, 504])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "# train_nodes, _ = get_graph_node_names(model) \n",
    "# print(\"\\n\".join(train_nodes))\n",
    "feature_extractor = create_feature_extractor(model, {'resnet.flatten':'features'})\n",
    "\n",
    "# # pre-select important feature\n",
    "vfeatures = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for _, vdata in enumerate(test_dataloader):\n",
    "        vinputs, vlabels = vdata['pixel_values'], vdata['labels'].cpu()\n",
    "        voutputs = feature_extractor(vinputs)\n",
    "        features = voutputs['features']\n",
    "        vfeatures = torch.cat((vfeatures, features.cpu()), 0)\n",
    "\n",
    "vfeatures_mean = np.mean(vfeatures.numpy(), axis=0) \n",
    "vfeatures_std = np.std(vfeatures.numpy(), axis=0) \n",
    "vfeature_imp_idx = np.where((vfeatures_mean > 0.8 ) & (vfeatures_std>0.8))[0]\n",
    "vfeature_imp_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 34.,  77., 110., 113.,  74.,  60.,  27.,  11.,   3.,   3.]),\n",
       " array([0.45543331, 0.56832445, 0.68121564, 0.79410678, 0.90699792,\n",
       "        1.01988912, 1.13278019, 1.24567139, 1.35856259, 1.47145367,\n",
       "        1.58434486]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vfeatures_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c552ce235d14d648694627d1ffcf5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c08d806d4394424879eb808bbab9435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3f3a323d4a4331b28243fb4e98fd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # first get model performance with all features\n",
    "# all_feature_performance = {'f1': 0.32909090909090905, 'roc_auc': 0.7106333347154663, 'accuracy': 0.056530214424951264}\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=32)\n",
    "all_feature_performance = evaluate(test_dataloader, threshold=thresholds, verbose=0)\n",
    "test_dataloader_male = DataLoader(test_ds_male, collate_fn=collate_fn, batch_size=32)\n",
    "all_feature_perf_male = evaluate(test_dataloader_male, threshold=thresholds, verbose=0)\n",
    "test_dataloader_female = DataLoader(test_ds_female, collate_fn=collate_fn, batch_size=32)\n",
    "all_feature_perf_female = evaluate(test_dataloader_female, threshold=thresholds, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184c2fef34e744b1b1b2998359e5ce6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Forward pass through the feature extractor\n",
    "model.eval()\n",
    "\n",
    "feature_shape = 512\n",
    "importances = []\n",
    "for i in tqdm(vfeature_imp_idx): # there are in total 512 features\n",
    "    masker = torch.ones(feature_shape, dtype=torch.float32).to(device)\n",
    "    masker[i] = 0.0\n",
    "    with torch.no_grad():\n",
    "        v_y_true = torch.tensor([], dtype=torch.long)\n",
    "        v_y_pred = torch.tensor([])\n",
    "        for _, vdata in enumerate(test_dataloader):\n",
    "            vinputs, vlabels = vdata['pixel_values'], vdata['labels'].cpu()\n",
    "            voutputs = feature_extractor(vinputs)\n",
    "            vfeatures_maked = voutputs['features'] * masker\n",
    "            vprobs = model.resnet.fc(vfeatures_maked)\n",
    "            v_y_pred = torch.cat((v_y_pred, vprobs.cpu()), 0)\n",
    "            v_y_true = torch.cat((v_y_true, vlabels), 0)\n",
    "        masked_preformance = multi_label_metrics(v_y_pred, v_y_true.numpy(), threshold=thresholds, verbose=0)\n",
    "        importances.append(all_feature_performance['f1'] - masked_preformance['f1'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029647829647829194"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances[np.argmax(importances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "gender_test_ds = [int(x['Patient Gender'] == 'M') for x in test_ds]\n",
    "\n",
    "# for i in range(vfeatures.shape[1]):\n",
    "#     vfeatures[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "features_pearsonr = []\n",
    "for i in range(512):\n",
    "    v = vfeatures.numpy()[:, i]\n",
    "    v = np.where(v > np.mean(v), 1.0, 0)\n",
    "    features_pearsonr.append(stats.pearsonr(gender_test_ds, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_pearsonr_v = [x[0] for x in features_pearsonr]\n",
    "plt.hist(features_pearsonr_v, bins='auto') \n",
    "vfeature_imp_idx = np.argsort(-np.array(features_pearsonr_v))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside forward hook for Linear\n",
      "Input shape: torch.Size([6, 2])\n",
      "Output shape: torch.Size([6, 4])\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0275,  0.3476, -0.1044, -0.4723,  0.2896],\n",
       "        [-0.0275,  0.3476, -0.1044, -0.4723,  0.2896],\n",
       "        [-0.0275,  0.3476, -0.1044, -0.4723,  0.2896],\n",
       "        [-0.0275,  0.3476, -0.1044, -0.4723,  0.2896],\n",
       "        [-0.0275,  0.3476, -0.1044, -0.4723,  0.2896],\n",
       "        [-0.0275,  0.3476, -0.1044, -0.4723,  0.2896]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ## An example to show how to perturbation the immediate feature.\n",
    "\n",
    "# from torch import nn\n",
    "\n",
    "# class cls_model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(cls_model,self).__init__()\n",
    "#         self.l1 = nn.Linear(3,2)\n",
    "#         self.l2 = nn.Linear(2,4)\n",
    "#         self.l3 = nn.Linear(4,5)\n",
    "#     def forward(self, x):\n",
    "#         y1 = self.l1(x)\n",
    "#         y2 = self.l2(y1)\n",
    "#         y3 = self.l3(y2)\n",
    "#         return y3\n",
    "    \n",
    "# def forward_hook(module, input, output):\n",
    "#     print(f\"Inside forward hook for {module.__class__.__name__}\")\n",
    "#     print(f\"Input shape: {input[0].shape}\")\n",
    "#     print(f\"Output shape: {output.shape}\")\n",
    "#     print(\"--------\")\n",
    "#     return torch.zeros(output.shape)\n",
    "# model_cls = cls_model()\n",
    "# hook_handle = model_cls.l2.register_forward_hook(forward_hook)\n",
    "# x = torch.randn((6,3))\n",
    "# model.forward(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
